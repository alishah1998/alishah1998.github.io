[
  {
    "objectID": "Resume.html",
    "href": "Resume.html",
    "title": "Resume",
    "section": "",
    "text": "alishah1998@hotmail.com • (469) 877-9009 • linkedin.com/in/alishah1998\nEDUCATION\nThe University of Texas at Dallas | M.S. in Social Data Analytics and Research | May 2025\nRelevant Courses: Methods of Data Collection, Machine Learning & Data Mining, Introduction to Python, Research Design, Information Management, Advanced Statistics\nThe University of Texas at Austin | Professional Certificate in Data Analytics Essential | April 2023\nRelevant Courses: Data Analytics with Excel, Data Analytics with SQL, Data Driven Insights using Python, Dashboarding with Tableau\nTECHNICAL SKILLS\nProgramming Languages:  SQL, R, Python, Tableau, Excel, HTML, C++, Azure\nPackages: Pandas, NumPy, Matplotlib, BeautifulSoup, ggplot, Tidyverse, ArcGIS, seaborn, sklearn, quarto\nSkills: Regression Models, Supervised Learning, Unsupervised Learning, Web Scrapping, Automation, Mean Square Error, Root Mean Square Error, R-Squared, Exploratory Data Analysis, Train & Test Models\nPROFESSIONAL EXPERIENCE\nDigital Business Analyst Career Experience | Apple Inc. | Jan 2024 – Jun 2024\n●      Collaborated with teams across multiple departments and developed a fully functional Digital Monitoring application that scrapes across multiple E-commerce channels to ensure brand compliance.\no   Increased compliance by 94% across all US Channels\n●      Analyzed data regarding online sentiments regarding Apple products.\no   Developed interactive dashboard for stakeholders.\n●      Develop compliance guidelines for US E-commerce Channels\nResearch Analyst Internship | Texas Division of Emergency Management | Aug 2023 – Dec 2023\n●      Assisted in social media data collection regarding emergencies posted on social media on Twitter, Instagram, Facebook, Reddit, Etc.\n●      Generate Incident reports and analysis which include response time, property damage, Emergency teams' deployment, and level of severity.\nResearch Analyst Assistant | The University of Texas at Dallas | Aug 2019 – Aug 2020\n●      Volunteered at the Healthy Development Project research lab under Dr. Shayla Holub\n●      Research included collection and aggregating data regarding child eating habits using Excel.\n●      Presented research findings during UT Dallas 2019 Research Fair"
  },
  {
    "objectID": "Resume.html#ali-m-shah",
    "href": "Resume.html#ali-m-shah",
    "title": "Resume",
    "section": "",
    "text": "alishah1998@hotmail.com • (469) 877-9009 • linkedin.com/in/alishah1998\nEDUCATION\nThe University of Texas at Dallas | M.S. in Social Data Analytics and Research | May 2025\nRelevant Courses: Methods of Data Collection, Machine Learning & Data Mining, Introduction to Python, Research Design, Information Management, Advanced Statistics\nThe University of Texas at Austin | Professional Certificate in Data Analytics Essential | April 2023\nRelevant Courses: Data Analytics with Excel, Data Analytics with SQL, Data Driven Insights using Python, Dashboarding with Tableau\nTECHNICAL SKILLS\nProgramming Languages:  SQL, R, Python, Tableau, Excel, HTML, C++, Azure\nPackages: Pandas, NumPy, Matplotlib, BeautifulSoup, ggplot, Tidyverse, ArcGIS, seaborn, sklearn, quarto\nSkills: Regression Models, Supervised Learning, Unsupervised Learning, Web Scrapping, Automation, Mean Square Error, Root Mean Square Error, R-Squared, Exploratory Data Analysis, Train & Test Models\nPROFESSIONAL EXPERIENCE\nDigital Business Analyst Career Experience | Apple Inc. | Jan 2024 – Jun 2024\n●      Collaborated with teams across multiple departments and developed a fully functional Digital Monitoring application that scrapes across multiple E-commerce channels to ensure brand compliance.\no   Increased compliance by 94% across all US Channels\n●      Analyzed data regarding online sentiments regarding Apple products.\no   Developed interactive dashboard for stakeholders.\n●      Develop compliance guidelines for US E-commerce Channels\nResearch Analyst Internship | Texas Division of Emergency Management | Aug 2023 – Dec 2023\n●      Assisted in social media data collection regarding emergencies posted on social media on Twitter, Instagram, Facebook, Reddit, Etc.\n●      Generate Incident reports and analysis which include response time, property damage, Emergency teams' deployment, and level of severity.\nResearch Analyst Assistant | The University of Texas at Dallas | Aug 2019 – Aug 2020\n●      Volunteered at the Healthy Development Project research lab under Dr. Shayla Holub\n●      Research included collection and aggregating data regarding child eating habits using Excel.\n●      Presented research findings during UT Dallas 2019 Research Fair"
  },
  {
    "objectID": "knowledge mining.html",
    "href": "knowledge mining.html",
    "title": "Assignment 2: Databases",
    "section": "",
    "text": "https://docs.google.com/presentation/d/1b_wvQ-EUrTsdPpbINc_yfYlScU_zrV3XQ658ejSZEwI/edit?usp=sharing"
  },
  {
    "objectID": "Assignment 8.html",
    "href": "Assignment 8.html",
    "title": "Assignment 8",
    "section": "",
    "text": "Setting up for analysis\nrequire(ISLR)\nrequire(MASS)\nrequire(descr)\nattach(Smarket)\nLinear Discriminant Analysis\nfreq(Direction) train = Year&lt;2005 lda.fit=lda(Direction~Lag1+Lag2,data=Smarket, subset=Year&lt;2005) lda.fit\n\ntrain = Year&lt;2005 lda.fit=lda(Direction~Lag1+Lag2,data=Smarket, subset=Year&lt;2005) lda.fit plot(lda.fit, col=\"dodgerblue\")"
  },
  {
    "objectID": "Assignment 6.html",
    "href": "Assignment 6.html",
    "title": "Assignment 6",
    "section": "",
    "text": "install.packages(c(\"easypackages\",\"MASS\",\"ISLR\",\"arm\")) library(easypackages) libraries(\"arm\",\"MASS\",\"ISLR\")"
  },
  {
    "objectID": "Assignment 6.html#load-datasets-from-mass-and-islr-packages",
    "href": "Assignment 6.html#load-datasets-from-mass-and-islr-packages",
    "title": "Assignment 6",
    "section": "Load datasets from MASS and ISLR packages",
    "text": "Load datasets from MASS and ISLR packages\nattach(Boston)\n\nSimple linear regression\nnames(Boston) # What is the Boston dataset? ?Boston plot(medv~lstat,Boston, pch=20, cex=.8, col=\"steelblue\") fit1=lm(medv~lstat,data=Boston) fit1 summary(fit1) abline(fit1,col=\"firebrick\") names(fit1) confint(fit1) # confidence intervals"
  },
  {
    "objectID": "Assignment 6.html#i-identity-function-for-squared-term-to-interpret-as-is",
    "href": "Assignment 6.html#i-identity-function-for-squared-term-to-interpret-as-is",
    "title": "Assignment 6",
    "section": "I() identity function for squared term to interpret as-is",
    "text": "I() identity function for squared term to interpret as-is"
  },
  {
    "objectID": "Assignment 6.html#combine-two-command-lines-with-semicolon",
    "href": "Assignment 6.html#combine-two-command-lines-with-semicolon",
    "title": "Assignment 6",
    "section": "Combine two command lines with semicolon",
    "text": "Combine two command lines with semicolon\nfit6=lm(medv~lstat +I(lstat^2),Boston); summary(fit6) par(mfrow=c(1,1)) plot(medv~lstat, pch=20, col=\"forestgreen\")\npoints(lstat,fitted(fit6),col=\"firebrick\",pch=20) fit7=lm(medv~poly(lstat,4)) points(lstat,fitted(fit7),col=\"steelblue\",pch=20)\n###Qualitative predictors names(Carseats) summary(Carseats) fit1=lm(Sales~.+Income:Advertising+Age:Price,Carseats) # add two interaction terms summary(fit1) attach(Carseats) contrasts(Carseats$ShelveLoc) # what is contrasts function? ?contrasts\n\nWriting an R function to combine the lm, plot and abline functions to\n\n\ncreate a one step regression fit plot function\nregplot=function(x,y){ fit=lm(y~x) plot(x,y, pch=20) abline(fit,col=\"firebrick\") } attach(Carseats) regplot(Price,Sales) ## Allow extra room for additional arguments/specifications regplot=function(x,y,...){ fit=lm(y~x) plot(x,y,...) abline(fit,col=\"firebrick\") } regplot(Price,Sales,xlab=\"Price\",ylab=\"Sales\",col=\"steelblue\",pch=20)"
  },
  {
    "objectID": "Assignment 6.html#additional-note-try-out-the-coefplot2-package-to-finetune-the-coefplots",
    "href": "Assignment 6.html#additional-note-try-out-the-coefplot2-package-to-finetune-the-coefplots",
    "title": "Assignment 6",
    "section": "Additional note: try out the coefplot2 package to finetune the coefplots",
    "text": "Additional note: try out the coefplot2 package to finetune the coefplots\n##install.packages(\"coefplot2\", repos=\"http://www.math.mcmaster.ca/bolker/R\", type=\"source\") ## library(coefplot2)"
  },
  {
    "objectID": "Assignment 4.html",
    "href": "Assignment 4.html",
    "title": "Assignment 4",
    "section": "",
    "text": "This is the output following after utilizing a k-means cluster and looking for the top 20 most commonly used phrases. Notably, “college” emerged as the most frequently used keyword. This aligns with the focus of our study, which centers on college-age individuals discussing depression. It’s widely acknowledged that the college years can present significant challenges as young adults navigate newfound independence. Another noteworthy keyword is “parents.” This finding is particularly relevant, as many South Asians grapple with a cultural disparity in their relationships with their parents. With a significant number of South Asians immigrating to the United States in recent years, the current college generation represents the first fully Americanized cohort. This cultural shift often results in conflicts stemming from differing expectations and values between parents and their children, particularly regarding family dynamics and lifestyle choices.\nSimilarly, the prevalence of the keyword “school” underscores the ongoing educational context of our study cohort. However, one unexpected observation was the prominence of the keyword “Mom” without a corresponding mention of “father.” We postulate that this discrepancy may be attributed to the traditional role of mothers as primary caregivers in many South Asian households. Such roles often lead to heightened tensions and conflicts between mothers and their children, potentially overshadowing paternal influences in discussions of familial dynamics and mental health."
  },
  {
    "objectID": "Assignment 2.html",
    "href": "Assignment 2.html",
    "title": "Assignment 2",
    "section": "",
    "text": "What problems do you encounter with new data sets?\nWorking with datasets often presents several challenges that can impede the analysis and modeling process. One of the most common issues is data quality, which encompasses problems like missing values, outliers, duplicates, and inaccuracies, all of which can skew results if not properly addressed. Data cleaning is a crucial step in preparing the dataset for analysis, involving tasks such as handling missing data, removing duplicates, and transforming variables to ensure consistency. Additionally, large datasets pose computational challenges due to their volume, requiring significant resources and processing time for tasks like training machine learning models or performing complex analyses. Integrating data from different sources or formats can be problematic, especially when dealing with incompatible structures or quality issues. Understanding the semantics and relationships within the dataset is essential for effective analysis but can be hindered by a lack of domain knowledge or data documentation. Furthermore, ensuring data privacy and security, addressing bias and fairness issues, and establishing proper data governance practices are all important considerations when working with datasets. Overall, addressing these challenges requires a combination of domain expertise, data management skills, and the use of appropriate tools and techniques for data analysis and manipulation.\nHow to deal with missing values?\nDealing with missing values is crucial in data preprocessing. Strategies include removing rows or columns, imputing values using techniques like mean or predictive imputation, flagging missingness, and incorporating domain knowledge. Careful evaluation is necessary to avoid bias, and the chosen method should suit the data and analysis goals.\nExplore the relationship between Tondu and other variables including female, DPP, age, income, edu, Taiwanese and Econ_worse. What methods would you use?\nI used data exploration by creating different graphs to better understand what I am looking at. I will also compare related variables to understand exactly what I am looking for"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Lab 2: Google Trends vs Web Scrapping with R",
    "section": "",
    "text": "Google Trend vs Web Scrapping with R\nGoogle trend has a much friendlier user interface making it much easier to use. In this example I performed a google trend search analyzing the interest over time for Donald Trump and Joe Biden. To perform this search, I went onto trends.google.com/trends/explore. In the box “Add a search term”, I typed in “Donald Trump” and press the enter key. A “+ Compare” button will appear to the right of the previous term where I typed in “Joe Biden”. I then changed the region into United States.\nFollowing these steps will prompt google to display an Interest over time graph and a compared breakdown by subregion. The second graph allows the user to change the subregion into metro and city.\n\n\n\n\n\n\n\n\n\n\nGoogle trends allows you to download the CSV file that contains the data used to create these graphs. Having this data allows you to generate your own graphs using tools such as Excel. For example:\n\n\n\n\n\nThis is a column table that makes it easier to compare search results from specific days of the year. This feature allows for the user to gain a lot of insight with relative ease.\nAnother way to gather this information Is to web scrape the data using R. To do this you will need to install the package “gtrendsR”. After installing the package, we need to write a bit of code\nplot(gtrends(c(“Donald Trump”, “Joe Biden”), geo = “US”, time = “today 12-m”))\ndata(“countries”)\nThis line of code is essentially doing this search for us and creating a plot to compare the search results. The resulting graph will look like this:\n\n\n\n\n\nThe benefit of using this method is the speed you can receive visualizes. You can change the category to search, geography, and timespan by changing the code to your desired results. This has its advantages when wanting to perform niche searches that are not available with Google Trends. For example, Google Trends will not allow the user to select multiple countries at once. By scrapping the data, the user can compare search results from multiple terms in multiple locations. For example, I can compare Donald Trump (US), Donald Trump (MX), Joe Biden (US), and Joe Biden (MX).\nThis extra freedom comes with a more difficult user interface that may be more difficult for users to learn when compared to Google Trends But having that extra freedom when conducting web scrapping can allow for the user to gain much more insights then they could with Google Trends."
  },
  {
    "objectID": "Assignment 1.html",
    "href": "Assignment 1.html",
    "title": "Assignment 1",
    "section": "",
    "text": "This is telling R that we want only the second column in the EuStockMarkets tibble, and we want to save it as a numeric vector we also wrap the line in the as.numeric() function to make sure the data are stored as numeric data\nstocks &lt;- as.numeric(EuStockMarkets[,2])\nExercise 1: Get the 90th element of the vector stocks. Save it to an object named nintey\nnintey &lt;- stocks[90] nintey\nExercise 2 Get the last element of the vector stocks. Save it to an object named last.\nlast &lt;- stocks[1860] last\nExercise 3 Make a copy of the vector stocks, and name it copy. Then delete the first five elements of copy\ncopy &lt;- stocks copy[6] copy &lt;- copy[-c(1:5)] copy[1]\nExercise 4 Get all the entries from stocks that are above the mean value of stocks. Save this new vector as above. Then, get all the entries from stocks that are below the mean. Save this new vector as below.\nabove &lt;- stocks[stocks &gt; mean(stocks)] below &lt;- stocks[stocks &lt; mean(stocks)] above below\nExercise 5 On how many days were the closing prices greater than 6,000?\ngreater &lt;- length(stocks[stocks &gt; 6000]) greater\nSave a time vector (1990 to 1998) for the stocks data and then create and name a tibble consisting of the vectors year and stocks. Helpful hint: You can use the time() command on the original EuStockMarkets tibble to extract year variables.\nyear &lt;- time(EuStockMarkets) stock_year &lt;- tibble::tibble(year, stocks)"
  },
  {
    "objectID": "Assignment 3.html",
    "href": "Assignment 3.html",
    "title": "Assignment 3",
    "section": "",
    "text": "R Programming (EDA)\n\n## Creating a function: regplot\n## Combine the lm, plot and abline functions to create a regression fit plot function\nregplot=function(x,y){\n  fit=lm(y~x)\n  plot(x,y)\n  abline(fit,col=\"red\")\n}\n\n\nattach(ISLR::Carseats)\nregplot(Price,Sales)\n\n\n\n## Allow extra room for additional arguments/specifications\nregplot=function(x,y,...){\n  fit=lm(y~x)\n  plot(x,y,...)\n  abline(fit,col=\"red\")\n}  # \"...\" is called ellipsis, which is designed to take any number of named or unnamed arguments.\nregplot(Price,Sales,xlab=\"Price\",ylab=\"Sales\",col=\"blue\",pch=20)\n\n\n\n\n(Adapted from Stackoverflow examples) (Objectives: Use plotly, reshape packages, interactive visualization)\n\nlibrary(tidyverse)\nlibrary(plotly)\ndata(iris)\nattach(iris)\n# Generate plot on three quantitative variables\niris_plot &lt;- plot_ly(iris,\n                     x = Sepal.Length,\n                     y = Sepal.Width,\n                     z = Petal.Length,\n                     type = \"scatter3d\",\n                     mode = \"markers\",\n                     size = 0.02)\niris_plot\n\n\n\n\n# Regression object\n\npetal_lm &lt;- lm(Petal.Length ~ 0 + Sepal.Length + Sepal.Width,\n               data = iris)\nlibrary(reshape2)\n\n#load data\n\npetal_lm &lt;- lm(Petal.Length ~ 0 + Sepal.Length + Sepal.Width,data = iris)\n\n# Setting resolution parameter\ngraph_reso &lt;- 0.05\n\n#Setup Axis\naxis_x &lt;- seq(min(iris$Sepal.Length), max(iris$Sepal.Length), by = graph_reso)\naxis_y &lt;- seq(min(iris$Sepal.Width), max(iris$Sepal.Width), by = graph_reso)\n\n# Regression surface\n# Rearranging data for plotting\npetal_lm_surface &lt;- expand.grid(Sepal.Length = axis_x,Sepal.Width = axis_y,KEEP.OUT.ATTRS = F)\npetal_lm_surface$Petal.Length &lt;- predict.lm(petal_lm, newdata = petal_lm_surface)\npetal_lm_surface &lt;- acast(petal_lm_surface, Sepal.Width ~ Sepal.Length, value.var = \"Petal.Length\")\nhcolors=c(\"orange\",\"blue\",\"green\")[iris$Species]\niris_plot &lt;- plot_ly(iris,\n                     x = ~Sepal.Length,\n                     y = ~Sepal.Width,\n                     z = ~Petal.Length,\n                     text = Species,\n                     type = \"scatter3d\",\n                     mode = \"markers\",\n                     marker = list(color = hcolors),\n                     size=0.02)\n# Add surface\niris_plot &lt;- add_trace(p = iris_plot,\n                       z = petal_lm_surface,\n                       x = axis_x,\n                       y = axis_y,\n                       type = \"surface\",mode = \"markers\",\n                       marker = list(color = hcolors))\niris_plot\n\n\n\n\n\n\n\nRegression object\n\npetal_lm &lt;- lm(Petal.Length ~ 0 + Sepal.Length + Sepal.Width, \n               data = iris)\nsummary(petal_lm)\n\n\nCall:\nlm(formula = Petal.Length ~ 0 + Sepal.Length + Sepal.Width, data = iris)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.70623 -0.51867 -0.08334  0.49844  1.93093 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \nSepal.Length  1.56030    0.04557   34.24   &lt;2e-16 ***\nSepal.Width  -1.74570    0.08709  -20.05   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.6869 on 148 degrees of freedom\nMultiple R-squared:  0.973, Adjusted R-squared:  0.9726 \nF-statistic:  2663 on 2 and 148 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "Assignment 5.html",
    "href": "Assignment 5.html",
    "title": "Assignment 5",
    "section": "",
    "text": "Unsupervised learning is a class of machine learning algorithms to identify patterns or grouping structure in the data. Unlike supervised learning which relies on “supervised” information such as the dependent variable to guide modeling, unsupervised learning seeks to explore the structure and possible groupings of unlabeled data. This information will be useful to provide pre-processor for supervised learning.\nUnsupervised learning has no explicit dependent variable of Y for prediction. Instead, the goal is to discover interesting patterns about the measurements on \\((X_{1}), (X_{2}), . . . , (X_{p})\\) and identify any subgroups among the observations.\nGenerally, in this section, the two general methods are introduced: Principal components analysis and Clustering.\n\n\nPrincipal Components Analysis (PCA) produces a low-dimensional representation of a dataset. It finds a sequence of linear combinations of the variables that have maximal variance, and are mutually uncorrelated.\nThe first principal component of a set of features \\((X_1, X_2, . . . , X_p)\\) is the normalized linear combination of the features:  \\[  Z_1 = \\phi_{11}X_1 +\\phi_{21}X_2 +...+\\phi_{p1}X_p \\] \nthat has the largest variance. By normalized, we mean that \\(\\sum_{j=1}^p\\phi_{j1}^2 = 1\\).\nThe elements \\((\\phi_{11}, . . . , \\phi_{p1})\\) are the loadings of the first principal component; together, the loadings make up the principal component loading vector, \\(\\phi_1= (\\phi_{11} \\phi_{21} ... \\phi_{p1})^T\\)\nWe constrain the loadings so that their sum of squares is equal to one, since otherwise setting these elements to be arbitrarily large in absolute value could result in an arbitrarily large variance.\n\n\n\n\n\nThe K-means clustering method is to partition the data points into k groups such that the sum of squares from points to the assigned cluster center in each group is minimized.\n\n\n\nHierarchical clustering is an alternative approach which does not require a pre-specified or a particular choice of \\((K)\\).\nHierarchical Clustering has an advantage that it produces a tree-based representation of the observations: Dendrogram\nA dendrogram is built starting from the leaves and combining clusters up to the trunk. The result of hierarchical clustering is a tree-based representation of the objects, which is also known as dendrogram. Observations can be subdivided into groups by cutting the dendrogram at a desired similarity level."
  },
  {
    "objectID": "Assignment 5.html#principal-component-analysis-pca",
    "href": "Assignment 5.html#principal-component-analysis-pca",
    "title": "Assignment 5",
    "section": "",
    "text": "Principal Components Analysis (PCA) produces a low-dimensional representation of a dataset. It finds a sequence of linear combinations of the variables that have maximal variance, and are mutually uncorrelated.\nThe first principal component of a set of features \\((X_1, X_2, . . . , X_p)\\) is the normalized linear combination of the features:  \\[  Z_1 = \\phi_{11}X_1 +\\phi_{21}X_2 +...+\\phi_{p1}X_p \\] \nthat has the largest variance. By normalized, we mean that \\(\\sum_{j=1}^p\\phi_{j1}^2 = 1\\).\nThe elements \\((\\phi_{11}, . . . , \\phi_{p1})\\) are the loadings of the first principal component; together, the loadings make up the principal component loading vector, \\(\\phi_1= (\\phi_{11} \\phi_{21} ... \\phi_{p1})^T\\)\nWe constrain the loadings so that their sum of squares is equal to one, since otherwise setting these elements to be arbitrarily large in absolute value could result in an arbitrarily large variance."
  },
  {
    "objectID": "Assignment 5.html#clustering",
    "href": "Assignment 5.html#clustering",
    "title": "Assignment 5",
    "section": "",
    "text": "The K-means clustering method is to partition the data points into k groups such that the sum of squares from points to the assigned cluster center in each group is minimized.\n\n\n\nHierarchical clustering is an alternative approach which does not require a pre-specified or a particular choice of \\((K)\\).\nHierarchical Clustering has an advantage that it produces a tree-based representation of the observations: Dendrogram\nA dendrogram is built starting from the leaves and combining clusters up to the trunk. The result of hierarchical clustering is a tree-based representation of the objects, which is also known as dendrogram. Observations can be subdivided into groups by cutting the dendrogram at a desired similarity level."
  },
  {
    "objectID": "Assignment 7.html",
    "href": "Assignment 7.html",
    "title": "Assignment 7",
    "section": "",
    "text": "Q1. What is/are the requirement(s) of LDA?\nLatent Dirichlet Allocation (LDA) is a widely used topic modeling technique applied to collections of text documents. Its primary requirement is that the input data consists of a corpus of text documents, where each document is represented as a bag-of-words, disregarding word order and considering only word frequency. LDA assumes that each document in the corpus is a mixture of multiple topics, and each topic is a distribution over words. To effectively apply LDA, preprocessing of the text data is essential, involving tasks such as tokenization, stop word removal, stemming or lemmatization, and other techniques to clean and standardize the text. Additionally, the number of topics to be discovered must be specified a priori, although techniques such as model selection or cross-validation can help determine an optimal number of topics. In summary, LDA requires input text data, representation of documents as bag-of-words, assumptions about document-topic and topic-word distributions, preprocessing of text data, and specification of the number of topics or methods to determine it.\nQ2. Difference between LDA and Log Function?\nLDA and Logistic Regression are distinct machine learning algorithms with different purposes. LDA is used for unsupervised topic modeling, uncovering latent topics within text documents, while Logistic Regression is a supervised algorithm for classification tasks. LDA focuses on revealing hidden structures, such as topics, in data, while Logistic Regression predicts outcomes based on input features. Additionally, LDA relies on Bayesian inference and assumes a generative process for document creation, while Logistic Regression uses optimization techniques to find model parameters and assumes a linear relationship between features and outcomes. In summary, LDA and Logistic Regression differ in objectives, outputs, training processes, and underlying assumptions.\nQ3. What is ROC?\nThe Receiver Operating Characteristic (ROC) curve is a graphical plot that illustrates the performance of a binary classification model across different thresholds. It displays the trade-off between the true positive rate (sensitivity) and the false positive rate (1 - specificity) as the discrimination threshold is varied. The ROC curve is particularly useful for evaluating and comparing the performance of different classification models. A model with higher sensitivity and lower false positive rate will have a curve that is closer to the top-left corner of the plot, indicating better performance. The area under the ROC curve (AUC-ROC) provides a single metric to quantify the overall performance of the model, with values ranging from 0 to 1. AUC-ROC values closer to 1 indicate better discrimination ability, while values around 0.5 suggest random performance. The ROC curve is widely used in various domains, including medicine, finance, and machine learning, to assess the predictive power of classification models and make informed decisions about their deployment."
  },
  {
    "objectID": "index.html#my-data-science-journey",
    "href": "index.html#my-data-science-journey",
    "title": "Welcome",
    "section": "My Data Science Journey:",
    "text": "My Data Science Journey:\nMaster’s student in Social Data Analytics, merging Data Science with social science. Certified in Data Analytics Essentials (University of Texas) with proficiency in SQL, Tableau, Excel, and Python. Actively seeking internships in Data Analytics, Business Analytics, Business Intelligence, or Data Science. Proven ability to excel in fast-paced environments, collect accurate data, and communicate professionally. Eager to contribute skills and experience to a new opportunity.\nLets connect:\nLinkedIn\nEmail: alishah1998@hotmail.com\nPhone: 469-877-9009"
  },
  {
    "objectID": "personal projects.html",
    "href": "personal projects.html",
    "title": "Personal Projects",
    "section": "",
    "text": "Pancreas Medical Device Recall Analysis\nIn this project, Python was employed for web scraping the open-source FDA database, specifically focusing on recalls of medical devices related to the pancreas. Key packages such as requests, BeautifulSoup, Matplotlib, seaborn, and math facilitated the analysis. The OpenFDA API expedited data retrieval, enabling swift exploratory data analysis to glean insights into medical device recalls.\nVideo Game Sales Insight and Analysis:\nThis Project involved creating an interactive dashboard that helps employees to monitor KPIs of the health of a subscription-based sales model. Skill used includes Dashboarding, Data Filters, Parameters for measurements, Data modification and transformation using calculation, quick table calculations, creating appropriate charts, and creating a Tableau Public Interface.\nFood-Hub Order Analysis:\nThis project involved analyzing data obtained from a food delivery app. Employed missing value treatments to clean the data, provided a comprehensive statistical summary with visual representations, and made business recommendations based on the findings. The skills utilized in this project include Variable Identification, Univariate and Bi-Variate analysis, as well as proficiency in Python programming.\nStress Related to Transitioning from Online to In-person During the COVID19 Pandemic:\nThis is an IRB approved research project conducted at the University of Texas at Dallas, involving an online survey administered to 100 students to measure their perceived stress before and after transitioning from online to in-person classes. The data collected, stored, and analyzed using Excel and ANNOV. Data was visualized in Excel and poster was produced in MS PowerPoint. The study was presented at a UT Dallas research fair. The study’s methodologies, procedure, data, limitations, and conclusions were documented in APA format."
  }
]