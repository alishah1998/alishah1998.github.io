[
  {
    "objectID": "index.html#my-data-science-journey",
    "href": "index.html#my-data-science-journey",
    "title": "Welcome",
    "section": "My Data Science Journey:",
    "text": "My Data Science Journey:\nI’m a data scientist based in Dallas, Texas with a Master of Science in Business Analytics & Artificial Intelligence from The University of Texas at Dallas. I’ve gained hands-on experience at leading companies like Apple, Texas Instruments, and Resultant, where I worked on projects involving data visualization, dashboarding, and machine learning. I specialize in turning complex data into clear, actionable insights using tools like SQL, Python, Tableau, and Power BI. I’m passionate about solving real-world problems through data and am actively seeking new opportunities in data analytics, business intelligence, and data science.\nLets connect:\nLinkedIn\nEmail: alishah1998@hotmail.com\nPhone: 469-877-9009"
  },
  {
    "objectID": "Resume.html",
    "href": "Resume.html",
    "title": "Resume",
    "section": "",
    "text": "alishah1998@hotmail.com • (469) 877-9009 • linkedin.com/in/alishah1998\nEDUCATION\nThe University of Texas at Dallas | M.S. in Business Analytics & Artificial Intelligence | May 2025\nRelevant Courses: Methods of Data Collection, Machine Learning & Data Mining, Introduction to Python, Research Design, Information Management, Advanced Statistics, AWS. Time-Series Analysis\nThe University of Texas at Austin | Professional Certificate in Data Analytics Essential | April 2023\nRelevant Courses: Data Analytics with Excel, Data Analytics with SQL, Data Driven Insights using Python, Dashboarding with Tableau\nThe University of Texas at Dallas| B.S. Psychology | May 2022\nRelevant Courses: Experimental Design, Marketing, Social Psychology, Advanced Statistics, Introduction to Programming, Advanced Calculus\nTECHNICAL SKILLS\nProgramming Languages:  SQL, R, Python, Tableau, Excel, HTML, C++, AWS\nPackages: Pandas, NumPy, Matplotlib, BeautifulSoup, ggplot, Tidyverse, ArcGIS, seaborn, sklearn\nSkills: Regression Models, Supervised Learning, Unsupervised Learning, Web Scrapping, Automation, Mean Square Error, Root Mean Square Error, R-Squared, Exploratory Data Analysis, A/B Testing, Business Intelligence Dashboard, ETL Pipelines\nPROFESSIONAL EXPERIENCE\nData Science Internship - Technology Consulting | Resultant | January 2025 – June 2025\n●     Designed and implemented an ETL pipeline using SQL and Python to collect, clean, and visualize H5N1 virus data in Tableau. Developed an interactive heatmap to track virus spread across Wyoming, enabling data-driven decision-making for state resource allocation.\n●      Developed an automated bidding intelligence tool using Python, BeautifulSoup, and NLP techniques powered by large language models (LLMs). The system performs daily web scraping, text extraction, and summarization to generate actionable insights, supporting contract targeting and increasing bid submissions.\nData Science Internship - People Analytics | Texas Instruments | May 2024 – August 2024\n●      Built and managed an ETL pipeline using SQL to maintain HR databases and developed regression models in Python (scikit-learn) to predict factors driving employee retention, improving model accuracy by 12%.\n●      Collected, cleaned, and analyzed HR data using SQL and Python (pandas, NumPy), and visualized findings in Tableau and Power BI to support data-driven decisions—boosting stakeholder understanding by 50%.\n●      Applied statistical modeling in Python to assess the impact of manager training programs, uncovering key performance insights. Visualized outcomes in Power BI, leading to a 10% improvement in training effectiveness.\nData Analytics Career Experience | Apple Inc | January 2024 – June 2024\n●      Led the development of an AI-powered digital monitoring application to automate web content review, resulting in a 94% improvement in brand compliance across all U.S. channels.\n●      Applied sentiment analysis and NLP techniques to identify misconceptions in online product discussions, improving product page clarity and driving process enhancements.\n●      Established standardized compliance guidelines for resellers on digital marketplaces, ensuring 100% brand consistency and contributing to increased digital sales.\n●      Built an end-to-end ETL pipeline using SQL and Python (pandas, NumPy) to aggregate and analyze data across U.S. carriers and channels. Leveraged Tableau to visualize trends, keyword behavior, and competitive insights—enhancing market understanding by 10%.",
    "crumbs": [
      "Resume"
    ]
  },
  {
    "objectID": "Resume.html#ali-m-shah",
    "href": "Resume.html#ali-m-shah",
    "title": "Resume",
    "section": "",
    "text": "alishah1998@hotmail.com • (469) 877-9009 • linkedin.com/in/alishah1998\nEDUCATION\nThe University of Texas at Dallas | M.S. in Business Analytics & Artificial Intelligence | May 2025\nRelevant Courses: Methods of Data Collection, Machine Learning & Data Mining, Introduction to Python, Research Design, Information Management, Advanced Statistics, AWS. Time-Series Analysis\nThe University of Texas at Austin | Professional Certificate in Data Analytics Essential | April 2023\nRelevant Courses: Data Analytics with Excel, Data Analytics with SQL, Data Driven Insights using Python, Dashboarding with Tableau\nThe University of Texas at Dallas| B.S. Psychology | May 2022\nRelevant Courses: Experimental Design, Marketing, Social Psychology, Advanced Statistics, Introduction to Programming, Advanced Calculus\nTECHNICAL SKILLS\nProgramming Languages:  SQL, R, Python, Tableau, Excel, HTML, C++, AWS\nPackages: Pandas, NumPy, Matplotlib, BeautifulSoup, ggplot, Tidyverse, ArcGIS, seaborn, sklearn\nSkills: Regression Models, Supervised Learning, Unsupervised Learning, Web Scrapping, Automation, Mean Square Error, Root Mean Square Error, R-Squared, Exploratory Data Analysis, A/B Testing, Business Intelligence Dashboard, ETL Pipelines\nPROFESSIONAL EXPERIENCE\nData Science Internship - Technology Consulting | Resultant | January 2025 – June 2025\n●     Designed and implemented an ETL pipeline using SQL and Python to collect, clean, and visualize H5N1 virus data in Tableau. Developed an interactive heatmap to track virus spread across Wyoming, enabling data-driven decision-making for state resource allocation.\n●      Developed an automated bidding intelligence tool using Python, BeautifulSoup, and NLP techniques powered by large language models (LLMs). The system performs daily web scraping, text extraction, and summarization to generate actionable insights, supporting contract targeting and increasing bid submissions.\nData Science Internship - People Analytics | Texas Instruments | May 2024 – August 2024\n●      Built and managed an ETL pipeline using SQL to maintain HR databases and developed regression models in Python (scikit-learn) to predict factors driving employee retention, improving model accuracy by 12%.\n●      Collected, cleaned, and analyzed HR data using SQL and Python (pandas, NumPy), and visualized findings in Tableau and Power BI to support data-driven decisions—boosting stakeholder understanding by 50%.\n●      Applied statistical modeling in Python to assess the impact of manager training programs, uncovering key performance insights. Visualized outcomes in Power BI, leading to a 10% improvement in training effectiveness.\nData Analytics Career Experience | Apple Inc | January 2024 – June 2024\n●      Led the development of an AI-powered digital monitoring application to automate web content review, resulting in a 94% improvement in brand compliance across all U.S. channels.\n●      Applied sentiment analysis and NLP techniques to identify misconceptions in online product discussions, improving product page clarity and driving process enhancements.\n●      Established standardized compliance guidelines for resellers on digital marketplaces, ensuring 100% brand consistency and contributing to increased digital sales.\n●      Built an end-to-end ETL pipeline using SQL and Python (pandas, NumPy) to aggregate and analyze data across U.S. carriers and channels. Leveraged Tableau to visualize trends, keyword behavior, and competitive insights—enhancing market understanding by 10%.",
    "crumbs": [
      "Resume"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Lab 2: Google Trends vs Web Scrapping with R",
    "section": "",
    "text": "Google Trend vs Web Scrapping with R\nGoogle trend has a much friendlier user interface making it much easier to use. In this example I performed a google trend search analyzing the interest over time for Donald Trump and Joe Biden. To perform this search, I went onto trends.google.com/trends/explore. In the box “Add a search term”, I typed in “Donald Trump” and press the enter key. A “+ Compare” button will appear to the right of the previous term where I typed in “Joe Biden”. I then changed the region into United States.\nFollowing these steps will prompt google to display an Interest over time graph and a compared breakdown by subregion. The second graph allows the user to change the subregion into metro and city.\n\n\n\n\n\n\n\n\n\n\nGoogle trends allows you to download the CSV file that contains the data used to create these graphs. Having this data allows you to generate your own graphs using tools such as Excel. For example:\n\n\n\n\n\nThis is a column table that makes it easier to compare search results from specific days of the year. This feature allows for the user to gain a lot of insight with relative ease.\nAnother way to gather this information Is to web scrape the data using R. To do this you will need to install the package “gtrendsR”. After installing the package, we need to write a bit of code\nplot(gtrends(c(“Donald Trump”, “Joe Biden”), geo = “US”, time = “today 12-m”))\ndata(“countries”)\nThis line of code is essentially doing this search for us and creating a plot to compare the search results. The resulting graph will look like this:\n\n\n\n\n\nThe benefit of using this method is the speed you can receive visualizes. You can change the category to search, geography, and timespan by changing the code to your desired results. This has its advantages when wanting to perform niche searches that are not available with Google Trends. For example, Google Trends will not allow the user to select multiple countries at once. By scrapping the data, the user can compare search results from multiple terms in multiple locations. For example, I can compare Donald Trump (US), Donald Trump (MX), Joe Biden (US), and Joe Biden (MX).\nThis extra freedom comes with a more difficult user interface that may be more difficult for users to learn when compared to Google Trends But having that extra freedom when conducting web scrapping can allow for the user to gain much more insights then they could with Google Trends."
  },
  {
    "objectID": "personal projects.html",
    "href": "personal projects.html",
    "title": "Personal Projects",
    "section": "",
    "text": "Pancreas Medical Device Recall Analysis\nIn this project, Python was employed for web scraping the open-source FDA database, specifically focusing on recalls of medical devices related to the pancreas. Key packages such as requests, BeautifulSoup, Matplotlib, seaborn, and math facilitated the analysis. The OpenFDA API expedited data retrieval, enabling swift exploratory data analysis to glean insights into medical device recalls.\nVideo Game Sales Insight and Analysis:\nThis Project involved creating an interactive dashboard that helps employees to monitor KPIs of the health of a subscription-based sales model. Skill used includes Dashboarding, Data Filters, Parameters for measurements, Data modification and transformation using calculation, quick table calculations, creating appropriate charts, and creating a Tableau Public Interface.\nFood-Hub Order Analysis:\nThis project involved analyzing data obtained from a food delivery app. Employed missing value treatments to clean the data, provided a comprehensive statistical summary with visual representations, and made business recommendations based on the findings. The skills utilized in this project include Variable Identification, Univariate and Bi-Variate analysis, as well as proficiency in Python programming.\nStress Related to Transitioning from Online to In-person During the COVID19 Pandemic:\nThis is an IRB approved research project conducted at the University of Texas at Dallas, involving an online survey administered to 100 students to measure their perceived stress before and after transitioning from online to in-person classes. The data collected, stored, and analyzed using Excel and ANNOV. Data was visualized in Excel and poster was produced in MS PowerPoint. The study was presented at a UT Dallas research fair. The study’s methodologies, procedure, data, limitations, and conclusions were documented in APA format."
  }
]