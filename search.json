[
  {
    "objectID": "Resume.html",
    "href": "Resume.html",
    "title": "Resume",
    "section": "",
    "text": "alishah1998@hotmail.com • (469) 877-9009 • linkedin.com/in/alishah1998\nEDUCATION\nThe University of Texas at Dallas | M.S. in Social Data Analytics and Research | May 2025\nRelevant Courses: Methods of Data Collection, Machine Learning & Data Mining, Introduction to Python, Research Design, Information Management, Advanced Statistics\nThe University of Texas at Austin | Professional Certificate in Data Analytics Essential | April 2023\nRelevant Courses: Data Analytics with Excel, Data Analytics with SQL, Data Driven Insights using Python, Dashboarding with Tableau\nTECHNICAL SKILLS\nProgramming Languages:  SQL, R, Python, Tableau, Excel, HTML, C++, Azure\nPackages: Pandas, NumPy, Matplotlib, BeautifulSoup, ggplot, Tidyverse, ArcGIS, seaborn, sklearn, quarto\nSkills: Regression Models, Supervised Learning, Unsupervised Learning, Web Scrapping, Automation, Mean Square Error, Root Mean Square Error, R-Squared, Exploratory Data Analysis, Train & Test Models\nPROFESSIONAL EXPERIENCE\nDigital Business Analyst Career Experience | Apple Inc. | Jan 2024 – Jun 2024\n●      Collaborated with teams across multiple departments and developed a fully functional Digital Monitoring application that scrapes across multiple E-commerce channels to ensure brand compliance.\no   Increased compliance by 94% across all US Channels\n●      Analyzed data regarding online sentiments regarding Apple products.\no   Developed interactive dashboard for stakeholders.\n●      Develop compliance guidelines for US E-commerce Channels\nResearch Analyst Internship | Texas Division of Emergency Management | Aug 2023 – Dec 2023\n●      Assisted in social media data collection regarding emergencies posted on social media on Twitter, Instagram, Facebook, Reddit, Etc.\n●      Generate Incident reports and analysis which include response time, property damage, Emergency teams' deployment, and level of severity.\nResearch Analyst Assistant | The University of Texas at Dallas | Aug 2019 – Aug 2020\n●      Volunteered at the Healthy Development Project research lab under Dr. Shayla Holub\n●      Research included collection and aggregating data regarding child eating habits using Excel.\n●      Presented research findings during UT Dallas 2019 Research Fair"
  },
  {
    "objectID": "Resume.html#ali-m-shah",
    "href": "Resume.html#ali-m-shah",
    "title": "Resume",
    "section": "",
    "text": "alishah1998@hotmail.com • (469) 877-9009 • linkedin.com/in/alishah1998\nEDUCATION\nThe University of Texas at Dallas | M.S. in Social Data Analytics and Research | May 2025\nRelevant Courses: Methods of Data Collection, Machine Learning & Data Mining, Introduction to Python, Research Design, Information Management, Advanced Statistics\nThe University of Texas at Austin | Professional Certificate in Data Analytics Essential | April 2023\nRelevant Courses: Data Analytics with Excel, Data Analytics with SQL, Data Driven Insights using Python, Dashboarding with Tableau\nTECHNICAL SKILLS\nProgramming Languages:  SQL, R, Python, Tableau, Excel, HTML, C++, Azure\nPackages: Pandas, NumPy, Matplotlib, BeautifulSoup, ggplot, Tidyverse, ArcGIS, seaborn, sklearn, quarto\nSkills: Regression Models, Supervised Learning, Unsupervised Learning, Web Scrapping, Automation, Mean Square Error, Root Mean Square Error, R-Squared, Exploratory Data Analysis, Train & Test Models\nPROFESSIONAL EXPERIENCE\nDigital Business Analyst Career Experience | Apple Inc. | Jan 2024 – Jun 2024\n●      Collaborated with teams across multiple departments and developed a fully functional Digital Monitoring application that scrapes across multiple E-commerce channels to ensure brand compliance.\no   Increased compliance by 94% across all US Channels\n●      Analyzed data regarding online sentiments regarding Apple products.\no   Developed interactive dashboard for stakeholders.\n●      Develop compliance guidelines for US E-commerce Channels\nResearch Analyst Internship | Texas Division of Emergency Management | Aug 2023 – Dec 2023\n●      Assisted in social media data collection regarding emergencies posted on social media on Twitter, Instagram, Facebook, Reddit, Etc.\n●      Generate Incident reports and analysis which include response time, property damage, Emergency teams' deployment, and level of severity.\nResearch Analyst Assistant | The University of Texas at Dallas | Aug 2019 – Aug 2020\n●      Volunteered at the Healthy Development Project research lab under Dr. Shayla Holub\n●      Research included collection and aggregating data regarding child eating habits using Excel.\n●      Presented research findings during UT Dallas 2019 Research Fair"
  },
  {
    "objectID": "knowledge mining.html",
    "href": "knowledge mining.html",
    "title": "Assignment 2: Databases",
    "section": "",
    "text": "https://docs.google.com/presentation/d/1b_wvQ-EUrTsdPpbINc_yfYlScU_zrV3XQ658ejSZEwI/edit?usp=sharing"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Lab 2: Google Trends vs Web Scrapping with R",
    "section": "",
    "text": "Google Trend vs Web Scrapping with R\nGoogle trend has a much friendlier user interface making it much easier to use. In this example I performed a google trend search analyzing the interest over time for Donald Trump and Joe Biden. To perform this search, I went onto trends.google.com/trends/explore. In the box “Add a search term”, I typed in “Donald Trump” and press the enter key. A “+ Compare” button will appear to the right of the previous term where I typed in “Joe Biden”. I then changed the region into United States.\nFollowing these steps will prompt google to display an Interest over time graph and a compared breakdown by subregion. The second graph allows the user to change the subregion into metro and city.\n\n\n\n\n\n\n\n\n\n\nGoogle trends allows you to download the CSV file that contains the data used to create these graphs. Having this data allows you to generate your own graphs using tools such as Excel. For example:\n\n\n\n\n\nThis is a column table that makes it easier to compare search results from specific days of the year. This feature allows for the user to gain a lot of insight with relative ease.\nAnother way to gather this information Is to web scrape the data using R. To do this you will need to install the package “gtrendsR”. After installing the package, we need to write a bit of code\nplot(gtrends(c(“Donald Trump”, “Joe Biden”), geo = “US”, time = “today 12-m”))\ndata(“countries”)\nThis line of code is essentially doing this search for us and creating a plot to compare the search results. The resulting graph will look like this:\n\n\n\n\n\nThe benefit of using this method is the speed you can receive visualizes. You can change the category to search, geography, and timespan by changing the code to your desired results. This has its advantages when wanting to perform niche searches that are not available with Google Trends. For example, Google Trends will not allow the user to select multiple countries at once. By scrapping the data, the user can compare search results from multiple terms in multiple locations. For example, I can compare Donald Trump (US), Donald Trump (MX), Joe Biden (US), and Joe Biden (MX).\nThis extra freedom comes with a more difficult user interface that may be more difficult for users to learn when compared to Google Trends But having that extra freedom when conducting web scrapping can allow for the user to gain much more insights then they could with Google Trends."
  },
  {
    "objectID": "Assignment 1.html",
    "href": "Assignment 1.html",
    "title": "Assignment 1: Introduction To R",
    "section": "",
    "text": "This is telling R that we want only the second column in the EuStockMarkets tibble, and we want to save it as a numeric vector we also wrap the line in the as.numeric() function to make sure the data are stored as numeric data\nstocks &lt;- as.numeric(EuStockMarkets[,2])\nExercise 1: Get the 90th element of the vector stocks. Save it to an object named nintey\nnintey &lt;- stocks[90] nintey\nExercise 2 Get the last element of the vector stocks. Save it to an object named last.\nlast &lt;- stocks[1860] last\nExercise 3 Make a copy of the vector stocks, and name it copy. Then delete the first five elements of copy\ncopy &lt;- stocks copy[6] copy &lt;- copy[-c(1:5)] copy[1]\nExercise 4 Get all the entries from stocks that are above the mean value of stocks. Save this new vector as above. Then, get all the entries from stocks that are below the mean. Save this new vector as below.\nabove &lt;- stocks[stocks &gt; mean(stocks)] below &lt;- stocks[stocks &lt; mean(stocks)] above below\nExercise 5 On how many days were the closing prices greater than 6,000?\ngreater &lt;- length(stocks[stocks &gt; 6000]) greater\nSave a time vector (1990 to 1998) for the stocks data and then create and name a tibble consisting of the vectors year and stocks. Helpful hint: You can use the time() command on the original EuStockMarkets tibble to extract year variables.\nyear &lt;- time(EuStockMarkets) stock_year &lt;- tibble::tibble(year, stocks)"
  },
  {
    "objectID": "index.html#my-data-science-journey",
    "href": "index.html#my-data-science-journey",
    "title": "Welcome",
    "section": "My Data Science Journey:",
    "text": "My Data Science Journey:\nMaster’s student in Social Data Analytics, merging Data Science with social science. Certified in Data Analytics Essentials (University of Texas) with proficiency in SQL, Tableau, Excel, and Python. Actively seeking internships in Data Analytics, Business Analytics, Business Intelligence, or Data Science. Proven ability to excel in fast-paced environments, collect accurate data, and communicate professionally. Eager to contribute skills and experience to a new opportunity.\nLets connect:\nLinkedIn\nEmail: alishah1998@hotmail.com\nPhone: 469-877-9009"
  },
  {
    "objectID": "personal projects.html",
    "href": "personal projects.html",
    "title": "Personal Projects",
    "section": "",
    "text": "Pancreas Medical Device Recall Analysis\nIn this project, Python was employed for web scraping the open-source FDA database, specifically focusing on recalls of medical devices related to the pancreas. Key packages such as requests, BeautifulSoup, Matplotlib, seaborn, and math facilitated the analysis. The OpenFDA API expedited data retrieval, enabling swift exploratory data analysis to glean insights into medical device recalls.\nVideo Game Sales Insight and Analysis:\nThis Project involved creating an interactive dashboard that helps employees to monitor KPIs of the health of a subscription-based sales model. Skill used includes Dashboarding, Data Filters, Parameters for measurements, Data modification and transformation using calculation, quick table calculations, creating appropriate charts, and creating a Tableau Public Interface.\nFood-Hub Order Analysis:\nThis project involved analyzing data obtained from a food delivery app. Employed missing value treatments to clean the data, provided a comprehensive statistical summary with visual representations, and made business recommendations based on the findings. The skills utilized in this project include Variable Identification, Univariate and Bi-Variate analysis, as well as proficiency in Python programming.\nStress Related to Transitioning from Online to In-person During the COVID19 Pandemic:\nThis is an IRB approved research project conducted at the University of Texas at Dallas, involving an online survey administered to 100 students to measure their perceived stress before and after transitioning from online to in-person classes. The data collected, stored, and analyzed using Excel and ANNOV. Data was visualized in Excel and poster was produced in MS PowerPoint. The study was presented at a UT Dallas research fair. The study’s methodologies, procedure, data, limitations, and conclusions were documented in APA format."
  }
]